{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "# sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "\n",
    "from torchvision.io import read_image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class Rxrx1(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir=None):\n",
    "        self.le_ = LabelEncoder()\n",
    "\n",
    "        self.root_dir = os.path.join(root_dir, \"rxrx1_v1.0\")\n",
    "        self.imgs_dir = os.path.join(self.root_dir, \"images\")\n",
    "        self.metadata = pd.read_csv(os.path.join(self.root_dir, \"metadata.csv\"))\n",
    "        self.le_.fit(self.metadata['cell_type'].unique())\n",
    "        self.metadata['cell_type'] = self.le_.transform(self.metadata['cell_type'])\n",
    "        self.items = [(os.path.join(self.imgs_dir, item.experiment, \"Plate\" + str(item.plate), item.well + '_s' +\n",
    "                       str(item.site) + '.png'), item.cell_type, item.sirna_id) for item in self.metadata.itertuples(index=False)]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path, cell_type, sirna_id = self.items[index]\n",
    "        return (read_image(img_path), cell_type, sirna_id)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "class SimCLR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimCLR, self).__init__()\n",
    "        self.backbone = models.resnet18(weights='DEFAULT')\n",
    "        self.backbone.fc = nn.Identity()  # fully-connected removed\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        projections = self.projection_head(features)\n",
    "        return projections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "from wilds import get_dataset\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from source.net import SimCLR\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "\n",
    "\n",
    "def display_configs(configs):\n",
    "    t = PrettyTable([\"Name\", \"Value\"])\n",
    "    t.align = \"r\"\n",
    "    for key, value in configs.items():\n",
    "        t.add_row([key, value])\n",
    "    print(t, flush=True)\n",
    "\n",
    "\n",
    "def load_device(config):\n",
    "    if config[\"device\"] == \"gpu\":\n",
    "        assert torch.cuda.is_available(), \"Notebook is not configured properly!\"\n",
    "        device = \"cuda:0\"\n",
    "        print(\n",
    "            \"Training network on {}\".format(torch.cuda.get_device_name(device=device))\n",
    "        )\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(torch.cuda.get_device_properties(i).name)\n",
    "\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device\n",
    "\n",
    "\n",
    "def download_dataset():\n",
    "    dataset = get_dataset(dataset=\"rxrx1\", download=True, root_dir=\"\")\n",
    "\n",
    "\n",
    "def contrastive_loss(features, device, temperature=0.5):\n",
    "    features = F.normalize(features, dim=1)\n",
    "    similarity_matrix = torch.mm(features, features.T) / temperature\n",
    "    batch_size = features.shape[0]\n",
    "    labels = torch.arange(batch_size).to(device)\n",
    "    loss = F.cross_entropy(similarity_matrix, labels)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def info_nce_loss(features, device, temperature=0.5):\n",
    "    \"\"\"\n",
    "    Implements Noise Contrastive Estimation loss as explained in the simCLR paper.\n",
    "    Actual code is taken from here https://github.com/sthalles/SimCLR/blob/master/simclr.py\n",
    "    Args:\n",
    "        - features: torch tensor of shape (2*N, D) where N is the batch size.\n",
    "            The first N samples are the original views, while the last\n",
    "            N are the modified views.\n",
    "        - device: torch device\n",
    "        - temperature: float\n",
    "    \"\"\"\n",
    "    n_views = 2\n",
    "    assert features.shape[0] % n_views == 0  # make sure shapes are correct\n",
    "    batch_size = features.shape[0] // n_views\n",
    "\n",
    "    labels = torch.cat([torch.arange(batch_size) for i in range(n_views)], dim=0)\n",
    "    labels = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    features = F.normalize(features, dim=1)\n",
    "\n",
    "    similarity_matrix = torch.matmul(features, features.T)\n",
    "    assert similarity_matrix.shape == (\n",
    "        n_views * batch_size, n_views * batch_size)\n",
    "    assert similarity_matrix.shape == labels.shape\n",
    "\n",
    "    # discard the main diagonal from both: labels and similarities matrix\n",
    "    mask = torch.eye(labels.shape[0], dtype=torch.bool).to(device)\n",
    "    labels = labels[~mask].view(labels.shape[0], -1)\n",
    "    similarity_matrix = similarity_matrix[~mask].view(similarity_matrix.shape[0], -1)\n",
    "    # assert similarity_matrix.shape == labels.shape\n",
    "\n",
    "    # select and combine multiple positives\n",
    "    positives = similarity_matrix[labels.bool()].view(labels.shape[0], -1)\n",
    "\n",
    "    # select only the negatives the negatives\n",
    "    negatives = similarity_matrix[~labels.bool()].view(similarity_matrix.shape[0], -1)\n",
    "\n",
    "    logits = torch.cat([positives, negatives], dim=1)\n",
    "    labels = torch.zeros(logits.shape[0], dtype=torch.long).to(device)\n",
    "\n",
    "    logits = logits / temperature\n",
    "    return F.cross_entropy(logits, labels)\n",
    "\n",
    "\n",
    "def config_loader(config):\n",
    "    net, loss, opt = ..., ..., ...\n",
    "    if str(config[\"net\"]).__contains__(\"simclr\"):\n",
    "        net = SimCLR()\n",
    "\n",
    "    if str(config[\"loss\"]).__contains__(\"contrastive\"):\n",
    "        loss = contrastive_loss\n",
    "    if str(config[\"loss\"]).__contains__(\"NCE\"):\n",
    "        loss = info_nce_loss\n",
    "\n",
    "    if str(config[\"opt\"]).__contains__(\"adam\"):\n",
    "        opt = torch.optim.Adam(net.parameters(), lr=0.005)\n",
    "\n",
    "    return (net, loss, opt)\n",
    "\n",
    "\n",
    "def test_kmean_accuracy(net, test_loader, device):\n",
    "    net.eval()\n",
    "    test_features = []\n",
    "    test_labels = []\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch, _ in tqdm(\n",
    "            test_loader\n",
    "        ):  # Suppongo tu abbia le etichette nel test set\n",
    "            features = net(\n",
    "                x_batch.to(torch.float).to(device)\n",
    "            )  # Estrazione delle feature\n",
    "            test_features.append(features)\n",
    "            test_labels.append(y_batch.to(device))\n",
    "\n",
    "    test_features = torch.cat(test_features).cpu()\n",
    "    test_labels = torch.cat(test_labels).cpu()\n",
    "    kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "    predicted_clusters = kmeans.fit_predict(test_features)\n",
    "    cluster_to_class = {}\n",
    "\n",
    "    for cluster_id in range(4):\n",
    "        indices = np.where(predicted_clusters == cluster_id)[0]\n",
    "        true_labels = test_labels[indices]\n",
    "        most_common_class = mode(true_labels).mode\n",
    "        cluster_to_class[cluster_id] = most_common_class\n",
    "\n",
    "    mapped_predictions = np.array([cluster_to_class[c] for c in predicted_clusters])\n",
    "    accuracy = np.mean(mapped_predictions == test_labels.numpy())\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "def validation_loss(net, val_loader, device, transform, std_transform, loss_func):\n",
    "    validation_loss_values = []\n",
    "    pbar = tqdm(total=len(val_loader), desc=f\"validation\")\n",
    "    with net.eval() and torch.no_grad():\n",
    "        for x_batch, _, _ in val_loader:\n",
    "            standard_views = torch.cat(\n",
    "                [std_transform(img.unsqueeze(0)) for img in x_batch], dim=0).to(device)\n",
    "            augmented_views = torch.cat(\n",
    "                [transform(img.unsqueeze(0)) for img in x_batch], dim=0).to(device)\n",
    "            block = torch.cat([standard_views, augmented_views], dim=0)\n",
    "            out_feat = net.forward(block.to(torch.float))\n",
    "            loss = loss_func(out_feat, device)\n",
    "\n",
    "            validation_loss_values.append(loss.item())\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({\"Validation Loss\": loss.item()})\n",
    "\n",
    "    return validation_loss_values\n",
    "\n",
    "\n",
    "def save_model(epoch, net, opt, train_loss, val_loss, batch_size, checkpoint_dir, optimizer):\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": net.state_dict(),\n",
    "            \"optimizer_state_dict\": opt.state_dict(),\n",
    "            \"training_loss_values\": train_loss,\n",
    "            \"validation_loss_values\": val_loss,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"optimizer\": optimizer,\n",
    "        },\n",
    "        os.path.join(checkpoint_dir, \"checkpoint{}\".format(epoch + 1)),\n",
    "    )\n",
    "\n",
    "def load_yaml(inFile):\n",
    "    with open(inFile, \"r\") as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    display_configs(config)\n",
    "\n",
    "    assert Path(config['checkpoint_dir']).is_dir(), \"Please provide a valid directory to save checkpoints in.\"\n",
    "    assert Path(config['dataset_dir']).is_dir(), \"Please provide a valid directory to load dataset.\"\n",
    "    if 'load_checkpoint' in config.keys():\n",
    "        assert Path(config['load_checkpoint']).is_dir(), \"Please provide a valid directory to load dataset.\"\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import torch, torchvision, yaml\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from pathlib import Path\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, net, device, config, opt, loss_func, scheduler=None):\n",
    "        self.net = net\n",
    "        self.device = device\n",
    "        self.config = config\n",
    "        self.opt = opt\n",
    "        self.loss_func = loss_func\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        checkpoint = torch.load(self.config['load_checkpoint'])\n",
    "        if self.config['multiple_gpus']:\n",
    "            model_dict = {key.replace(\n",
    "                \"module.\", \"\"): value for key, value in checkpoint['model_state_dict'].items()}\n",
    "            self.net.load_state_dict(model_dict)\n",
    "        else:\n",
    "            self.net.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        self.opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        try:\n",
    "            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        except:\n",
    "            print(\"Scheduler not found in the checkpoint.\")\n",
    "\n",
    "        last_epoch = checkpoint['epoch']\n",
    "        training_loss_values = checkpoint['training_loss_values']\n",
    "        validation_loss_values = checkpoint['validation_loss_values']\n",
    "        config['batch_size'] = checkpoint['batch_size']\n",
    "        return (last_epoch, training_loss_values, validation_loss_values)\n",
    "\n",
    "    def train(self, split_sizes, dataset, transform, std_transform):\n",
    "\n",
    "        train_size = int(split_sizes[0] * len(dataset))\n",
    "        val_size = int(split_sizes[1] * len(dataset))\n",
    "        test_size = len(dataset) - train_size - val_size\n",
    "        train_dataset, val_dataset, test_dataset = random_split(\n",
    "            dataset, [train_size, val_size, test_size])\n",
    "\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset, batch_size=config['batch_size'])\n",
    "\n",
    "        if 'load_checkpoint' in self.config.keys():\n",
    "            print('Loading latest checkpoint... ')\n",
    "            last_epoch, training_loss_values, validation_loss_values = self.load_checkpoint()\n",
    "            print(f\"Checkpoint {config['load_checkpoint']} Loaded\")\n",
    "        else:\n",
    "            last_epoch = 0\n",
    "            training_loss_values = []  # store every training loss value\n",
    "            validation_loss_values = []  # store every validation loss value\n",
    "\n",
    "        self.net = self.net.to(device)\n",
    "        self.net.train()\n",
    "        if self.config['multiple_gpus']:\n",
    "            self.net = nn.DataParallel(self.net)\n",
    "\n",
    "        for epoch in range(last_epoch, int(self.config['epochs'])):\n",
    "            pbar = tqdm(total=len(train_dataloader), desc=f\"Epoch-{epoch}\")\n",
    "            for x_batch, _, _ in train_dataloader:\n",
    "                standard_views = torch.cat(\n",
    "                    [std_transform(img.unsqueeze(0)) for img in x_batch], dim=0).to(device)\n",
    "                augmented_views = torch.cat(\n",
    "                    [transform(img.unsqueeze(0)) for img in x_batch], dim=0).to(device)\n",
    "                block = torch.cat([standard_views, augmented_views], dim=0)\n",
    "                out_feat = self.net.forward(block.to(torch.float))\n",
    "                loss = self.loss_func(out_feat, device)\n",
    "\n",
    "                self.opt.zero_grad()\n",
    "                loss.backward()\n",
    "                self.opt.step()\n",
    "                training_loss_values.append(loss.item())\n",
    "\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({'Loss': loss.item()})\n",
    "\n",
    "            if (epoch + 1) % int(self.config['model_save_freq']) == 0:\n",
    "                save_model(epoch, self.net, self.opt, training_loss_values, validation_loss_values,\n",
    "                           self.config['batch_size'], self.config['checkpoint_dir'], self.config['opt'])\n",
    "                test_kmean_accuracy(self.net.backbone, DataLoader(\n",
    "                    test_dataset, batch_size=self.config['batch_size']), self.device)\n",
    "\n",
    "            if (epoch + 1) % int(config['evaluation_freq']) == 0:\n",
    "                print(f\"Running Validation...\")\n",
    "                validation_loss_values += validation_loss(self.net, DataLoader(val_dataset, batch_size=self.config['batch_size'], pin_memory_device=self.device, pin_memory=True,\n",
    "                                                                               shuffle=True, num_workers=4, drop_last=True, prefetch_factor=1), self.device, transform, std_transform, self.loss_func)\n",
    "\n",
    "        return training_loss_values, validation_loss_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------------------------------+\n",
      "|            Name |                                     Value |\n",
      "+-----------------+-------------------------------------------+\n",
      "|      batch_size |                                       128 |\n",
      "|     dataset_dir |                    /work/ai4bio2024/rxrx1 |\n",
      "|  checkpoint_dir | /work/ai4bio2024/rxrx1/checkpoints/simclr |\n",
      "|          device |                                       gpu |\n",
      "|          epochs |                                         4 |\n",
      "|             net |                                    simclr |\n",
      "|            loss |                                       NCE |\n",
      "|             opt |                                      adam |\n",
      "| evaluation_freq |                                         4 |\n",
      "| model_save_freq |                                         4 |\n",
      "|   multiple_gpus |                                      True |\n",
      "+-----------------+-------------------------------------------+\n",
      "Training network on Tesla P100-PCIE-16GB\n",
      "Tesla P100-PCIE-16GB\n",
      "Tesla P100-PCIE-16GB\n",
      "Tesla P100-PCIE-16GB\n",
      "Tesla P100-PCIE-16GB\n",
      "Tesla P100-PCIE-16GB\n",
      "Tesla P100-PCIE-16GB\n",
      "Tesla P100-PCIE-16GB\n",
      "Tesla P100-PCIE-16GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation: 100%|██████████| 49/49 [00:43<00:00,  1.13it/s, Validation Loss=5.43]\n",
      "Epoch-0:   0%|          | 0/687 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    config = load_yaml(\"/homes/nmorelli/AIBIO_proj/config/train/server_conf.yaml\")\n",
    "    device = load_device(config)\n",
    "    dataset = Rxrx1(config['dataset_dir'])\n",
    "    net, loss_func, opt = config_loader(config)\n",
    "\n",
    "    # no transformations\n",
    "    std_transform = transforms.Compose(\n",
    "        [transforms.ToImage(), transforms.ToDtype(torch.float32, scale=True)])\n",
    "    # view for self supervised learning\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(256),\n",
    "        transforms.ColorJitter(brightness=0.5, contrast=0.5),\n",
    "        transforms.GaussianBlur(3, sigma=(0.1, 2.0)),\n",
    "        transforms.Compose([transforms.ToImage(), transforms.ToDtype(torch.float32, scale=True)])\n",
    "    ])\n",
    "\n",
    "    tr_ = Trainer(net, device, config, opt, loss_func)\n",
    "\n",
    "    training_loss_values, validation_loss_values = tr_.train([0.7, 0.05, 0.15], dataset, transform, std_transform)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
